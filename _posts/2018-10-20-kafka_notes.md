# kafka 生产者
几个关键流程
- brokers发现
- 序列化为字节数组并做压缩
- 消息批次
- 发送失败自动重试
- 发送模式：发送并忘记、同步发送、异步发送
- broker确认消息

## broker发现
**不需要**提供全部的brokers地址，kafka生产者可以从可用的brokers发现其他brokers，建议设置两个brokers，其中一个宕机时仍然可以连接到集群上

> 思考：是否可以定期刷新brokers，比如brokers集群配置有变化，最早发现的brokers地址无效（或者部分无效），通过重新发现来保持同步。或者由brokers集群主动通知生产者？

## 序列化和压缩
brokers接受字节数组，所以要把发送的对象序列化为字节。
消息发送默认不会压缩，可以指定 `compression.type = snappy|gzip|lz4` 进行压缩。
- snappy: google发明，占用较少CPU，可观的压缩比，比较关注性能和网络带宽可以考虑
- gzip: 占用较多CPU，提供更高的压缩比，适合网络带宽比较有限的情况
使用压缩可以降低网络和存储开销，这是kafka的主要瓶颈所在

## 消息批次
当有多个消息需要被发送到同一分区，会被放到同一个批次里，`batch.size` 定义了一个批次可以使用的内存大小（按字节），当批次满时该批次所有消息会被发送出去。但是并不是一定等批次满了才发送消息，所以`batch.size`设置得大了只会增加一些内存开销；设置得太小会导致消息被频繁发送出去。
`linger.ms` 参数则用来定义一个批次在发送出去前等待更多消息加入批次的时间
所以，生产者在批次填满或等待到达上限就会把批次的消息发送出去

本质上说，消息批次起到了类似 socket 缓冲区的作用。针对网络时延比较大的消息发送/接收，设置较大的缓冲区能够减少消息发送的网络开销，因为消息是一个批次一个批次发送的，批次还在网络中传输的时间正好可以用来累积消息数据，减少网络上传输的批次数量还能减少网络拥塞，但是这样也增加了消息传递的延迟

## 发送失败自动重试
通过设置 `retries` 重发次数，生产者在收到服务器临时错误时（网络抖动、分区找不到首领等），可以重发消息
默认每次重试会等待100ms，可以通过 `retry.backoff.ms` 来调试等待时长，不能太短，否则生产者会提前放弃重试，建议让总的重试时间（`retries * retry.backoff.ms`）比kafka集群恢复的时间长
由于有了自动重试，代码可以不用处理那些可重试的错误，只需要处理那些不可重试错误或者重试次数超出上限的情况

我们知道kafka可以保证分区内消息是有序的，即生产者按一定顺序发送消息，brokers会按这个顺序写入分区，消费者也按这个顺序读取。
但是如果设置了重试，则可能出现批次间乱序的问题，例如当 `max.in.flight.requests.per.connection > 1` 时，由于第一个批次发送失败，但是第二次批次可能写入成功，第一个批次被重试，两个批次顺序就反过来了。
将 `max.in.flight.requests.per.connection` 设置为 1，这样生产者尝试发送第一批消息时，就不会有其他消息发送给broker。但这样会严重影响生产者吞吐量，只有对消息顺序有严格要求时才这么做。将 `max.in.flight.requests.per.connection` 设置得较高会占用较多内存，因为同时需要为多个批次分配内存

> Q: 如果要保证生产者发送、broker写入分区、消费者读取的顺序均一致，即使没有发生重试，也需要设置 `max.in.flight.requests.per.connection = 1`才能达到？

## 发送模式
支持一下三种：
- 发送并忘记：业务代码不关心消息是否到达，大多数情况下消息会正常到达，而且生产者会自动尝试重发
- 同步发送：发送并返回一个future，调用future的`get`方法进行等待
- 异步发送：采用回调函数

参数 `acks` 指定了必须有多少个分区副本收到消息，生产者才会认为消息写入是成功的
- `acks == 0`: 生产者不等待服务器返回，也意味着生产者不会重试，会发生消息丢失，但是能够支持最大吞吐
- `acks == 1`: 生产者等待收到分区首领的响应，如果发生错误，生产者可以重试，但是如果发生首领重新选举且是一个没有收到消息的broker成为新首领，消息还是会丢失。采用同步发送模式，吞吐会比较受影响；采用异步发送模式，延迟问题可以得到缓解，但是还是会受到发送中消息数量的限制（`max.in.flight.requests.per.connection`、`buffer.memory`）

## 参考文章
1. [提高 Linux 上 socket 性能](https://www.ibm.com/developerworks/cn/linux/l-hisock.html)
2. [TCP的滑动窗口与拥塞窗口](https://blog.csdn.net/zhangdaisylove/article/details/47294315)
3. [tcp滑动窗口以拥塞窗口和各种缓冲的总结](https://blog.csdn.net/lishanmin11/article/details/77092652?utm_source=blogxgwz1)

# Brokers
一个独立的kafka服务器被称作broker。负责 1) 接手生产者的消息，为消息设置偏移量并提交到磁盘保存；2) 为消费者提供服务
一个broker可以轻松处理数千个分区和每秒百万级的消息量

## 集群控制器
每个集群都有一个broker充当**集群控制器**负责集群的管理，包括分区分配和扩容、监控等等，是由活跃的broker自动选举出来的。

## 消息保留策略
kafka broker支持按保留时间+保留消息字节数保存磁盘中的消息。当消息数量达到上限，旧消息就会过期并被删除。每个主题可以设置自己的保留策略。
kafka消息是按分区的日志片段保存的，一个日志片段一个文件，分配在由参数 `log.dirs` 指定的目录中，相同分区的日志文件在相同的目录中。
决定一个日志片段文件被关闭（不再写入消息数据）的包含两个参数 `log.segment.bytes` 和 `log.segment.ms`

决定一个主题的消息保留策略的关键参数是：
- `log.retention.ms`：消息数据可以保留多久，这是通过检查分区日志片段的**最后修改时间**（一般即最后一条消息的时间戳）来实现的。但是使用管理工具移动分区的时候由于会修改文件最后修改时间，所以此时会有些不准确
- `log.retension.bytes`：**作用在一个分区上**，表示该分区所有**已经关闭的**日志片段文件的数据总和上限
注意这两个参数都是作用于日志片段上，而且一个日志片段在**被关闭之前是不会过期的**

## 多数据中心消息复制
kafka的消息复制机制只能在单个集群内进行，不能在多集群之间进行。
> 多集群应该是指逻辑上的划分而不是物理上的划分？
可以使用 **MirrorMaker** 这个工具进行集群间的消息复制，其中内置了一个消费者和一个生产者，负责从一个集群消费消息，并通过
生产者发送到另一个集群中，整个过程是异步的，所以会存在数据一致性的问题
使用MirrorMaker可以实现集群间双向复制，实现双活的kafka集群，参考[这篇文章](https://www.altoros.com/blog/multi-cluster-deployment-options-for-apache-kafka-pros-and-cons/)。

## 系统资源使用
磁盘性能影响生产者，内存影响消费者。

Kafka依赖I/O性能为生产者提供快速的响应，这里主要考虑**脏页**的回写磁盘的影响。通过操作系统参数 `vm.dirty_background_ratio` 和 `vm.dirty_ratio` 来控制脏页刷新的频率，前者是针对后台刷新进程，后者针对内核进程，值单位为系统内存的百分比。调高参数值，降低了脏页刷新的频率，充分利用内核为磁盘提供缓冲的能力，但是也会带来因系统奔溃导致数据丢失的风险。
此外，禁用文件系统中对文件元数据 atime 的修改也可以提高性能（可以针对挂载点修改 `noatime`），kafka不需要用到文件最后被访问时间。

kafka的jvm本身不需要太多内存，剩余的系统内存可以用作页面缓存。消费者读取的消息往往直接从页面缓存能读到（消费者紧随在生产者后消费数据的场景下），而不用从磁盘上重新读取。
所以不建议kafka和其他重要的应用程序混合部署，因为这样就要共享页面缓存，最终降低消费的性能。

网络也是限制kafka性能的重要因素，因为给定一个分区，多消费使得消费的网络流量存在放大的情况，而且也会影响集群复制的延迟


# 消费者
## 分区再均衡
当一个消费者群组中，1）加入新的消费者；或者 2）有消费者退出（死亡或者主动退出）时，会触发分区再均衡。原本由某些消费者负责的分区移交给另外一些消费者负责

在再均衡期间，消费者无法读取消息（Q：所有消费者都无法读取消息？）
再均衡完成后，分区的新所有者消费者会从之前的偏移量开始读取消息，以尽量保证消息不被重复处理

## 心跳机制
消费者像被指定为群组协调器（group coordinator）的broker发送心跳来维持其和群组的从属关系以及对其分区的所有权
`hearbeat.interval.ms` 控制心跳发送间隔；`session.timeout.ms` 表示消费者在被认为死亡前可以多久不发送心跳

## 偏移量提交
默认可以采用自动每隔`autu.commit.interval.ms`自动提交偏移量。但是这样有可能导致消息被重复处理：假设5秒提交一次，在3秒的时候消费者崩溃导致再均衡，消费者从最后一次提交的偏移量开始读取消息，这3秒内到达的消息会被重复处理

可以选择通过API进行手动提交，又分位同步和异步两种思路。
异步提交要考虑偏移量覆盖的情况，比如先准备提交1000，但是失败了，期间又准备提交2000且提交成功，此时重复提交1000会导致2000的被覆盖。此时增加一个全局的偏移量状态值，每次提交前检查当前偏移量是否大于该状态值，若是则更新，成功就覆盖该状态值，否则不更新。






